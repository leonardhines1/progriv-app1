"""
Error Parser â€” Ñ€Ğ¾Ğ·Ğ±Ñ–Ñ€ CSV-Ñ„Ğ°Ğ¹Ğ»Ñ–Ğ² Ğ· Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ°Ğ¼Ğ¸ Ğ²Ñ–Ğ´ Google Ads.

Google Ads Editor ĞµĞºÑĞ¿Ğ¾Ñ€Ñ‚ÑƒÑ” CSV Ğ· Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ.
Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚: ĞºĞ¾Ğ¶ĞµĞ½ Ñ€ÑĞ´Ğ¾Ğº â€” Ñ†Ğµ Ñ€ÑĞ´Ğ¾Ğº ĞºĞ°Ğ¼Ğ¿Ğ°Ğ½Ñ–Ñ— (Campaign / Ad group / Keyword / Ad),
Ğ· ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¾Ñ "Results" Ğ°Ğ±Ğ¾ "Comment" Ğ°Ğ±Ğ¾ "Error" Ğ´Ğµ Ğ¼Ñ–ÑÑ‚Ğ¸Ñ‚ÑŒÑÑ Ğ¾Ğ¿Ğ¸Ñ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ¸.

ĞŸĞ°Ñ€ÑĞµÑ€ Ğ·Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ²Ñ–Ğ´Ñ…Ğ¸Ğ»ĞµĞ½Ñ– Ñ€ÑĞ´ĞºĞ¸ Ñ– Ğ²Ğ¸Ñ‚ÑĞ³ÑƒÑ”:
- Keywords (Ğ· Ñ€ÑĞ´ĞºÑ–Ğ² "Keyword") â†’ Ñ‚Ğ¸Ğ¿ "keyword"
- Headlines (Ğ· Ñ€ÑĞ´ĞºÑ–Ğ² "Responsive search ad") â†’ Ñ‚Ğ¸Ğ¿ "headline"
- Descriptions (Ğ· Ñ€ÑĞ´ĞºÑ–Ğ² "Responsive search ad") â†’ Ñ‚Ğ¸Ğ¿ "description"

Ğ¡Ğ°Ğ¼Ğ¾Ğ½Ğ°Ğ²Ñ‡Ğ°Ğ»ÑŒĞ½Ğ° Ğ»Ğ¾Ğ³Ñ–ĞºĞ°:
  Ğ’Ñ–Ğ´Ñ…Ğ¸Ğ»ĞµĞ½Ğµ keyword â†’ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ¾Ğ´Ğ°Ñ”Ñ‚ÑŒÑÑ Ğ² Banned Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ
  Ğ’Ñ–Ğ´Ñ…Ğ¸Ğ»ĞµĞ½Ğ¸Ğ¹ headline â†’ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·ÑƒÑ”Ñ‚ÑŒÑÑ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ° â†’ Ğ¿Ğ°Ñ‚ĞµÑ€Ğ½Ğ¸ Ğ´Ğ¾Ğ´Ğ°ÑÑ‚ÑŒÑÑ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ
"""

import csv
import os
import re
from typing import NamedTuple


class ParsedError(NamedTuple):
    """ĞĞ´Ğ½Ğ° Ñ€Ğ¾Ğ·Ğ¿Ñ–Ğ·Ğ½Ğ°Ğ½Ğ° Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ· Ñ„Ğ°Ğ¹Ğ»Ñƒ."""
    type: str          # "keyword" | "headline" | "description" | "campaign" | "ad_group"
    value: str         # Ñ‚ĞµĞºÑÑ‚ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ ĞµĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°
    reason: str        # Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ° / Ñ‚ĞµĞºÑÑ‚ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ¸ Ğ· Google Ads
    original_error: str  # Ğ¿Ğ¾Ğ²Ğ½Ğ¸Ğ¹ Ğ¾Ñ€Ğ¸Ğ³Ñ–Ğ½Ğ°Ğ»ÑŒĞ½Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ¸
    row_type: str      # "Keyword", "Responsive search ad", etc.


class ParseResult(NamedTuple):
    """Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ñƒ Ğ²ÑÑŒĞ¾Ğ³Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ñƒ."""
    errors: list          # list[ParsedError]
    total_rows: int       # Ğ²ÑÑŒĞ¾Ğ³Ğ¾ Ñ€ÑĞ´ĞºÑ–Ğ² Ñƒ Ñ„Ğ°Ğ¹Ğ»Ñ–
    error_rows: int       # Ñ€ÑĞ´ĞºÑ–Ğ² Ğ· Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ°Ğ¼Ğ¸
    success_rows: int     # ÑƒÑĞ¿Ñ–ÑˆĞ½Ğ¸Ñ… Ñ€ÑĞ´ĞºÑ–Ğ²
    filename: str         # Ñ–Ğ¼'Ñ Ñ„Ğ°Ğ¹Ğ»Ñƒ
    keywords: list        # list[ParsedError] â€” Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ keywords
    headlines: list       # list[ParsedError] â€” Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ headlines
    descriptions: list    # list[ParsedError] â€” Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ descriptions
    other_errors: list    # list[ParsedError] â€” Ñ–Ğ½ÑˆÑ– Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ¸


# â”€â”€â”€ ĞšĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸, Ğ´Ğµ Ğ¼Ğ¾Ğ¶Ğµ Ğ±ÑƒÑ‚Ğ¸ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ° â”€â”€â”€
ERROR_COLUMNS = [
    "Results", "Result", "Comment", "Error", "Error Details",
    "Validation Error", "Status", "Policy",
    # Google Ads Editor export columns
    "result", "results", "comment", "error",
]

# â”€â”€â”€ Ğ¡Ğ»Ğ¾Ğ²Ğ°, Ñ‰Ğ¾ Ğ²ĞºĞ°Ğ·ÑƒÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºÑƒ â”€â”€â”€
ERROR_INDICATORS = [
    "error", "rejected", "disapproved", "policy violation",
    "not eligible", "violation", "invalid", "too long",
    "restricted", "trademark", "misleading", "unacceptable",
    "failed", "couldn't", "couldn't create", "not allowed",
    "limit exceeded", "character limit", "exceeds",
]

# â”€â”€â”€ Ğ¡Ğ»Ğ¾Ğ²Ğ°, Ñ‰Ğ¾ Ğ²ĞºĞ°Ğ·ÑƒÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑĞ¿Ñ–Ñ… (Ñ–Ğ³Ğ½Ğ¾Ñ€ÑƒÑ”Ğ¼Ğ¾ Ñ†Ñ– Ñ€ÑĞ´ĞºĞ¸) â”€â”€â”€
SUCCESS_INDICATORS = [
    "successfully", "success", "created", "added", "updated",
    "approved", "eligible", "active", "enabled",
]


def _detect_delimiter(filepath: str) -> str:
    """Ğ’Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ñ” Ñ€Ğ¾Ğ·Ğ´Ñ–Ğ»ÑŒĞ½Ğ¸Ğº CSV (ĞºĞ¾Ğ¼Ğ°, ĞºÑ€Ğ°Ğ¿ĞºĞ° Ğ· ĞºĞ¾Ğ¼Ğ¾Ñ, Ğ°Ğ±Ğ¾ Ñ‚Ğ°Ğ±)."""
    with open(filepath, 'r', encoding='utf-8', errors='replace') as f:
        sample = f.read(2048)

    # ĞŸÑ–Ğ´Ñ€Ğ°Ñ…ÑƒĞ½Ğ¾Ğº
    counts = {
        ',': sample.count(','),
        ';': sample.count(';'),
        '\t': sample.count('\t'),
    }

    # Ğ¯ĞºÑ‰Ğ¾ Ñ‚Ğ°Ğ±Ñ–Ğ² Ğ±Ñ–Ğ»ÑŒÑˆĞµ â€” Ñ†Ğµ TSV
    if counts['\t'] > counts[','] and counts['\t'] > counts[';']:
        return '\t'
    if counts[';'] > counts[',']:
        return ';'
    return ','


def _normalize_header(header: str) -> str:
    """ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ñ–Ğ·ÑƒÑ” Ğ½Ğ°Ğ·Ğ²Ñƒ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑˆÑƒĞºÑƒ."""
    return header.strip().lower().replace(" ", "_").replace("-", "_")


def _find_error_column(headers: list) -> str | None:
    """Ğ—Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºÑƒ Ğ· Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ°Ğ¼Ğ¸ ÑĞµÑ€ĞµĞ´ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºÑ–Ğ²."""
    normalized = {_normalize_header(h): h for h in headers}

    # ĞŸÑ€Ñ–Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾ÑˆÑƒĞº
    priority = ["results", "result", "error", "error_details",
                 "comment", "validation_error", "policy", "status"]

    for key in priority:
        if key in normalized:
            return normalized[key]

    # Ğ¤Ğ¾Ğ»Ğ»Ğ±ĞµĞº: ÑˆÑƒĞºĞ°Ñ”Ğ¼Ğ¾ Ğ±ÑƒĞ´ÑŒ-ÑĞºÑƒ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºÑƒ Ğ·Ñ– ÑĞ»Ğ¾Ğ²Ğ¾Ğ¼ error/result/comment
    for norm, orig in normalized.items():
        if any(w in norm for w in ["error", "result", "comment", "policy", "validation"]):
            return orig

    return None


def _is_error_row(error_text: str) -> bool:
    """Ğ§Ğ¸ Ñ” Ñ†ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ¾Ñ (Ğ° Ğ½Ğµ ÑƒÑĞ¿Ñ–Ñ…Ğ¾Ğ¼)."""
    text_lower = error_text.lower()

    # Ğ¯ĞºÑ‰Ğ¾ Ñ” ÑĞ²Ğ½Ğ¸Ğ¹ Ñ–Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ ÑƒÑĞ¿Ñ–Ñ…Ñƒ â€” Ğ½Ğµ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ°
    for s in SUCCESS_INDICATORS:
        if s in text_lower and not any(e in text_lower for e in ["error", "rejected", "disapproved", "violation"]):
            return False

    # Ğ¯ĞºÑ‰Ğ¾ Ñ” Ñ–Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ¸ â€” Ñ†Ğµ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ°
    for e in ERROR_INDICATORS:
        if e in text_lower:
            return True

    # Ğ¯ĞºÑ‰Ğ¾ Ñ‚ĞµĞºÑÑ‚ Ğ½Ğµ Ğ¿Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ–Ğ¹ Ñ– Ğ½Ğµ ÑÑ…Ğ¾Ğ¶Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑĞ¿Ñ–Ñ… â€” Ğ²Ğ²Ğ°Ğ¶Ğ°Ñ”Ğ¼Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ¾Ñ
    # (Google Ads Ğ·Ğ°Ğ·Ğ²Ğ¸Ñ‡Ğ°Ğ¹ Ğ½Ğµ Ğ¿Ğ¸ÑˆĞµ Ğ½Ñ–Ñ‡Ğ¾Ğ³Ğ¾ Ğ´Ğ»Ñ ÑƒÑĞ¿Ñ–ÑˆĞ½Ğ¸Ñ… Ñ€ÑĞ´ĞºÑ–Ğ²)
    return len(text_lower.strip()) > 0


def _extract_reason(error_text: str) -> str:
    """Ğ’Ğ¸Ñ‚ÑĞ³ÑƒÑ” Ñ‡Ğ¸ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñƒ Ğ· Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ¸."""
    text = error_text.strip()

    # Ğ¯ĞºÑ‰Ğ¾ Ñ” ":" â€” Ğ±ĞµÑ€ĞµĞ¼Ğ¾ Ñ‡Ğ°ÑÑ‚Ğ¸Ğ½Ñƒ Ğ¿Ñ–ÑĞ»Ñ
    if ": " in text:
        parts = text.split(": ", 1)
        if len(parts[1]) > 10:
            text = parts[1]

    # Ğ¡ĞºĞ¾Ñ€Ğ¾Ñ‡ÑƒÑ”Ğ¼Ğ¾ Ğ´Ğ¾ 200 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ–Ğ²
    if len(text) > 200:
        text = text[:200] + "..."

    return text


def _extract_headline_errors(row: dict, error_text: str) -> list:
    """Ğ’Ğ¸Ñ‚ÑĞ³ÑƒÑ” Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ¾Ğ²Ñ– headlines Ğ· Ñ€ÑĞ´ĞºĞ° Responsive search ad."""
    errors = []
    error_lower = error_text.lower()
    reason = _extract_reason(error_text)

    for i in range(1, 16):
        col = f"Headline {i}"
        val = (row.get(col) or "").strip()
        if not val:
            continue

        # ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€ÑÑ”Ğ¼Ğ¾ Ñ‡Ğ¸ headline Ğ·Ğ³Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¿Ğ¾Ğ¼Ğ¸Ğ»Ñ†Ñ–
        is_specific = val.lower() in error_lower or f"headline {i}" in error_lower

        # Ğ¯ĞºÑ‰Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ·Ğ°Ğ³Ğ°Ğ»ÑŒĞ½Ğ° (policy violation) â€” Ğ´Ğ¾Ğ´Ğ°Ñ”Ğ¼Ğ¾ Ğ²ÑÑ– headlines
        is_general = any(w in error_lower for w in [
            "policy", "trademark", "misleading", "restricted",
            "disapproved", "rejected"
        ])

        if is_specific or is_general:
            errors.append(ParsedError(
                type="headline",
                value=val,
                reason=reason,
                original_error=error_text,
                row_type="Responsive search ad"
            ))

    return errors


def _extract_description_errors(row: dict, error_text: str) -> list:
    """Ğ’Ğ¸Ñ‚ÑĞ³ÑƒÑ” Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ¾Ğ²Ñ– descriptions Ğ· Ñ€ÑĞ´ĞºĞ° Responsive search ad."""
    errors = []
    error_lower = error_text.lower()
    reason = _extract_reason(error_text)

    for i in range(1, 5):
        col = f"Description {i}"
        val = (row.get(col) or "").strip()
        if not val:
            continue

        is_specific = val.lower() in error_lower or f"description {i}" in error_lower
        is_general = any(w in error_lower for w in [
            "policy", "trademark", "misleading", "restricted",
            "disapproved", "rejected"
        ])

        if is_specific or is_general:
            errors.append(ParsedError(
                type="description",
                value=val,
                reason=reason,
                original_error=error_text,
                row_type="Responsive search ad"
            ))

    return errors


def parse_error_csv(filepath: str) -> ParseResult:
    """
    Ğ“Ğ¾Ğ»Ğ¾Ğ²Ğ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ñ–Ñ: Ñ€Ğ¾Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ” CSV Ñ„Ğ°Ğ¹Ğ» Ğ· Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ°Ğ¼Ğ¸ Google Ads.

    ĞŸÑ–Ğ´Ñ‚Ñ€Ğ¸Ğ¼ÑƒÑ” Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸:
    1. Google Ads Editor â€” export results (Ğ¼Ğ°Ñ” ĞºĞ¾Ğ»Ğ¾Ğ½ĞºÑƒ Results/Error)
    2. Bulk upload results â€” has Result/Comment column
    3. ĞĞ°Ñˆ Ğ²Ğ»Ğ°ÑĞ½Ğ¸Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ CSV (Ğ· Row Type, Keyword, Headline Ñ‚Ğ¾Ñ‰Ğ¾)

    Returns: ParseResult Ğ· Ğ¿Ğ¾Ğ²Ğ½Ğ¸Ğ¼ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·Ğ¾Ğ¼
    """
    if not os.path.isfile(filepath):
        raise FileNotFoundError(f"Ğ¤Ğ°Ğ¹Ğ» Ğ½Ğµ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾: {filepath}")

    filename = os.path.basename(filepath)
    delimiter = _detect_delimiter(filepath)

    all_errors = []
    total_rows = 0
    error_rows = 0
    success_rows = 0

    with open(filepath, 'r', encoding='utf-8', errors='replace') as f:
        reader = csv.DictReader(f, delimiter=delimiter)
        headers = reader.fieldnames or []

        # Ğ—Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºÑƒ Ğ· Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ°Ğ¼Ğ¸
        error_col = _find_error_column(headers)

        # Ğ¯ĞºÑ‰Ğ¾ Ğ½ĞµĞ¼Ğ°Ñ” ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»Ğ¾Ğº â€” Ñ†Ğµ Ğ¼Ğ¾Ğ¶Ğµ Ğ±ÑƒÑ‚Ğ¸ Ğ½Ğ°Ñˆ CSV Ğ±ĞµĞ· Ğ¿Ğ¾Ğ¼Ğ¸Ğ»Ğ¾Ğº
        # Ğ¡Ğ¿Ñ€Ğ¾Ğ±ÑƒÑ”Ğ¼Ğ¾ Ñ–Ğ½ÑˆĞ¸Ğ¹ Ğ¿Ñ–Ğ´Ñ…Ñ–Ğ´: ÑˆÑƒĞºĞ°Ñ”Ğ¼Ğ¾ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºÑƒ "Row Type"
        has_row_type = any(_normalize_header(h) == "row_type" for h in headers)

        for row in reader:
            total_rows += 1

            # Ğ¢ĞµĞºÑÑ‚ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ¸
            error_text = ""
            if error_col:
                error_text = (row.get(error_col) or "").strip()

            # Ğ¯ĞºÑ‰Ğ¾ Ğ½ĞµĞ¼Ğ°Ñ” ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»Ğ¾Ğº Ñ– Ğ½ĞµĞ¼Ğ°Ñ” Row Type â€” Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ”Ğ¼Ğ¾
            if not error_text and not has_row_type:
                success_rows += 1
                continue

            # Ğ¯ĞºÑ‰Ğ¾ Ñ” Ñ‚ĞµĞºÑÑ‚ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ¸ â€” Ğ¿ĞµÑ€ĞµĞ²Ñ–Ñ€ÑÑ”Ğ¼Ğ¾
            if error_text and not _is_error_row(error_text):
                success_rows += 1
                continue

            if not error_text:
                success_rows += 1
                continue

            # â”€â”€â”€ Ğ Ğ¾Ğ·Ğ±Ñ–Ñ€ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ¸ Ğ·Ğ° Ñ‚Ğ¸Ğ¿Ğ¾Ğ¼ Ñ€ÑĞ´ĞºĞ° â”€â”€â”€
            error_rows += 1
            row_type = (row.get("Row Type") or row.get("row type") or
                        row.get("Type") or row.get("type") or "").strip()
            reason = _extract_reason(error_text)

            if row_type.lower() in ("keyword", "keywords"):
                kw = (row.get("Keyword") or row.get("keyword") or "").strip()
                if kw:
                    all_errors.append(ParsedError(
                        type="keyword",
                        value=kw,
                        reason=reason,
                        original_error=error_text,
                        row_type=row_type
                    ))

            elif row_type.lower() in ("responsive search ad", "ad", "text ad"):
                # Headlines
                all_errors.extend(_extract_headline_errors(row, error_text))
                # Descriptions
                all_errors.extend(_extract_description_errors(row, error_text))

                # Ğ¯ĞºÑ‰Ğ¾ Ğ½Ğµ Ğ·Ğ½Ğ°Ğ¹ÑˆĞ»Ğ¸ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ– â€” Ğ´Ğ¾Ğ´Ğ°Ñ”Ğ¼Ğ¾ Ğ·Ğ°Ğ³Ğ°Ğ»ÑŒĞ½Ñƒ Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºÑƒ Ğ¾Ğ³Ğ¾Ğ»Ğ¾ÑˆĞµĞ½Ğ½Ñ
                if not _extract_headline_errors(row, error_text) and \
                   not _extract_description_errors(row, error_text):
                    all_errors.append(ParsedError(
                        type="ad",
                        value=f"[{row_type}] Ad error",
                        reason=reason,
                        original_error=error_text,
                        row_type=row_type
                    ))

            elif row_type.lower() in ("campaign",):
                all_errors.append(ParsedError(
                    type="campaign",
                    value=row.get("Campaign") or "Unknown campaign",
                    reason=reason,
                    original_error=error_text,
                    row_type=row_type
                ))

            elif row_type.lower() in ("ad group", "ad_group"):
                all_errors.append(ParsedError(
                    type="ad_group",
                    value=row.get("Ad group") or "Unknown ad group",
                    reason=reason,
                    original_error=error_text,
                    row_type=row_type
                ))

            else:
                # ĞĞµĞ²Ñ–Ğ´Ğ¾Ğ¼Ğ¸Ğ¹ Ñ‚Ğ¸Ğ¿ â€” Ğ²ÑĞµ Ğ¾Ğ´Ğ½Ğ¾ Ğ·Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ”Ğ¼Ğ¾
                # Ğ¡Ğ¿Ñ€Ğ¾Ğ±ÑƒÑ”Ğ¼Ğ¾ Ğ·Ğ½Ğ°Ğ¹Ñ‚Ğ¸ keyword Ñƒ Ğ±ÑƒĞ´ÑŒ-ÑĞºÑ–Ğ¹ ĞºĞ¾Ğ»Ğ¾Ğ½Ñ†Ñ–
                kw = (row.get("Keyword") or row.get("keyword") or "").strip()
                if kw:
                    all_errors.append(ParsedError(
                        type="keyword",
                        value=kw,
                        reason=reason,
                        original_error=error_text,
                        row_type=row_type or "Unknown"
                    ))
                else:
                    all_errors.append(ParsedError(
                        type="other",
                        value=error_text[:100],
                        reason=reason,
                        original_error=error_text,
                        row_type=row_type or "Unknown"
                    ))

    # â”€â”€â”€ ĞšĞ»Ğ°ÑĞ¸Ñ„Ñ–ĞºĞ°Ñ†Ñ–Ñ â”€â”€â”€
    keywords = [e for e in all_errors if e.type == "keyword"]
    headlines = [e for e in all_errors if e.type == "headline"]
    descriptions = [e for e in all_errors if e.type == "description"]
    other_errors = [e for e in all_errors if e.type not in ("keyword", "headline", "description")]

    return ParseResult(
        errors=all_errors,
        total_rows=total_rows,
        error_rows=error_rows,
        success_rows=success_rows,
        filename=filename,
        keywords=keywords,
        headlines=headlines,
        descriptions=descriptions,
        other_errors=other_errors,
    )


def errors_to_submission(parsed: ParseResult, action: str = "auto_ban") -> list:
    """
    ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚ÑƒÑ” ParseResult Ñƒ ÑĞ¿Ğ¸ÑĞ¾Ğº dict-Ñ–Ğ² Ğ´Ğ»Ñ Ğ²Ñ–Ğ´Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ Ğ½Ğ° Sheet.

    action:
        "auto_ban" â€” keywords Ğ¾Ğ´Ñ€Ğ°Ğ·Ñƒ Ğ² Banned (Ğ¾Ğ±Ñ…Ñ–Ğ´ Pending)
        "pending"  â€” keywords Ğ² Pending Changes (Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ñ–Ñ)

    Returns: [{"type": "keyword", "value": "...", "reason": "...",
               "original_error": "...", "action": "auto_ban"}, ...]
    """
    submissions = []
    seen = set()  # Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ñ–ĞºĞ°Ñ†Ñ–Ñ

    for error in parsed.errors:
        # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ”Ğ¼Ğ¾ Ğ½ĞµĞ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ– Ñ‚Ğ¸Ğ¿Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ°Ğ½Ñƒ
        if error.type not in ("keyword", "headline", "description"):
            continue

        key = (error.type, error.value.lower())
        if key in seen:
            continue
        seen.add(key)

        submissions.append({
            "type": error.type,
            "value": error.value,
            "reason": f"Google Ads: {error.reason}",
            "original_error": error.original_error[:500],
            "action": action,
        })

    return submissions


def format_summary(parsed: ParseResult) -> str:
    """Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ÑƒÑ” ĞºÑ€Ğ°ÑĞ¸Ğ²Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¸Ğ¹ Ğ·Ğ²Ñ–Ñ‚ Ğ´Ğ»Ñ GUI."""
    lines = [
        f"ğŸ“„ Ğ¤Ğ°Ğ¹Ğ»: {parsed.filename}",
        f"ğŸ“Š Ğ’ÑÑŒĞ¾Ğ³Ğ¾ Ñ€ÑĞ´ĞºÑ–Ğ²: {parsed.total_rows}",
        f"âœ… Ğ£ÑĞ¿Ñ–ÑˆĞ½Ğ¸Ñ…: {parsed.success_rows}",
        f"âŒ Ğ— Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ°Ğ¼Ğ¸: {parsed.error_rows}",
        "",
        f"ğŸ”‘ Keywords Ğ· Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ°Ğ¼Ğ¸: {len(parsed.keywords)}",
        f"ğŸ“ Headlines Ğ· Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ°Ğ¼Ğ¸: {len(parsed.headlines)}",
        f"ğŸ“„ Descriptions Ğ· Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ°Ğ¼Ğ¸: {len(parsed.descriptions)}",
        f"âš ï¸ Ğ†Ğ½ÑˆÑ– Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ¸: {len(parsed.other_errors)}",
    ]

    if parsed.keywords:
        lines.append("\nâ”€â”€ Ğ’Ñ–Ğ´Ñ…Ğ¸Ğ»ĞµĞ½Ñ– Keywords â”€â”€")
        for kw in parsed.keywords[:20]:
            lines.append(f"  ğŸš« {kw.value}  â†  {kw.reason[:60]}")

    if parsed.headlines:
        lines.append("\nâ”€â”€ Ğ’Ñ–Ğ´Ñ…Ğ¸Ğ»ĞµĞ½Ñ– Headlines â”€â”€")
        for h in parsed.headlines[:10]:
            lines.append(f"  ğŸš« {h.value}  â†  {h.reason[:60]}")

    if parsed.descriptions:
        lines.append("\nâ”€â”€ Ğ’Ñ–Ğ´Ñ…Ğ¸Ğ»ĞµĞ½Ñ– Descriptions â”€â”€")
        for d in parsed.descriptions[:10]:
            lines.append(f"  ğŸš« {d.value[:50]}...  â†  {d.reason[:60]}")

    total_to_ban = len(parsed.keywords) + len(parsed.headlines) + len(parsed.descriptions)
    lines.append(f"\nğŸ¯ Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾ Ğ´Ğ¾ Ğ²Ñ–Ğ´Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ Ğ² Banned: {total_to_ban}")

    return "\n".join(lines)
